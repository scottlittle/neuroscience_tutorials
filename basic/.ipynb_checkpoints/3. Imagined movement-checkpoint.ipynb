{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imagined movement\n",
    "\n",
    "In this tutorial we will look at imagined movement. Our movement is controlled in the motor cortex where there is an increased level of mu activity (8–12 Hz) when we perform movements. This is accompanied by a reduction of this mu activity in specific regions that deal with the limb that is currently moving. This decrease is called Event Related Desynchronization (ERD). By measuring the amount of mu activity at different locations on the motor cortex, we can determine which limb the subject is moving. Through mirror neurons, this effect also occurs when the subject is not actually moving his limbs, but merely imagining it.\n",
    "\n",
    "## Credits\n",
    "The CSP code was originally written by Boris Reuderink of the Donders\n",
    "Institute for Brain, Cognition and Behavior. It is part of his Python EEG\n",
    "toolbox: https://github.com/breuderink/eegtools\n",
    "\n",
    "Inspiration for this tutorial also came from the excellent code example\n",
    "given in the book chapter:\n",
    " \n",
    "Arnaud Delorme, Christian Kothe, Andrey Vankov, Nima Bigdely-Shamlo,\n",
    "Robert Oostenveld, Thorsten Zander, and Scott Makeig. MATLAB-Based Tools\n",
    "for BCI Research, In _(B+H)CI: The Human in Brain-Computer Interfaces and\n",
    "the Brain in Human-Computer Interaction._ Desney S. Tan and Anton Nijholt\n",
    "(eds.), 2009, 241-259, http://dx.doi.org/10.1007/978-1-84996-272-8\n",
    "\n",
    "## Obtaining the data\n",
    "The dataset for this tutorial is provided by the fourth BCI competition,\n",
    "which you will have to download youself. First, go to http://www.bbci.de/competition/iv/#download\n",
    "and fill in your name and email address. An email will be sent to you\n",
    "automatically containing a username and password for the download area.\n",
    "\n",
    "Download Data Set 1, from Berlin, the 100Hz version in MATLAB format:\n",
    "http://bbci.de/competition/download/competition_iv/BCICIV_1_mat.zip\n",
    "and unzip it in a subdirectory called 'data_set_IV'. This subdirectory\n",
    "should be inside the directory in which you've store the tutorial files.\n",
    "\n",
    "[Description of the data](http://bbci.de/competition/iv/desc_1.html)\n",
    "\n",
    "If you've followed the instructions above, the following code should load\n",
    "the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data_set_IV/BCICIV_calib_ds1d.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-31c3be1850f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_set_IV/BCICIV_calib_ds1d.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct_as_record\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# SciPy.io.loadmat does not deal well with Matlab structures, resulting in lots of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scott/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m    133\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mMR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scott/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m        \u001b[0mtype\u001b[0m \u001b[0mdetected\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mbyte_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mmjv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scott/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data_set_IV/BCICIV_calib_ds1d.mat'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "m = scipy.io.loadmat('data_set_IV/BCICIV_calib_ds1d.mat', struct_as_record=True)\n",
    "\n",
    "# SciPy.io.loadmat does not deal well with Matlab structures, resulting in lots of\n",
    "# extra dimensions in the arrays. This makes the code a bit more cluttered\n",
    "\n",
    "sample_rate = m['nfo']['fs'][0][0][0][0]\n",
    "EEG = m['cnt'].T\n",
    "nchannels, nsamples = EEG.shape\n",
    "\n",
    "channel_names = [s[0].encode('utf8') for s in m['nfo']['clab'][0][0][0]]\n",
    "event_onsets = m['mrk'][0][0][0]\n",
    "event_codes = m['mrk'][0][0][1]\n",
    "labels = np.zeros((1, nsamples), int)\n",
    "labels[0, event_onsets] = event_codes\n",
    "\n",
    "cl_lab = [s[0].encode('utf8') for s in m['nfo']['classes'][0][0][0]]\n",
    "cl1 = cl_lab[0]\n",
    "cl2 = cl_lab[1]\n",
    "nclasses = len(cl_lab)\n",
    "nevents = len(event_onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data in the following python variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print some information\n",
    "print 'Shape of EEG:', EEG.shape\n",
    "print 'Sample rate:', sample_rate\n",
    "print 'Number of channels:', nchannels\n",
    "print 'Channel names:', channel_names\n",
    "print 'Number of events:', len(event_onsets)\n",
    "print 'Event codes:', np.unique(event_codes)\n",
    "print 'Class labels:', cl_lab\n",
    "print 'Number of classes:', nclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large recording: 118 electrodes where used, spread across the entire scalp. The subject was given a cue and then imagined either right hand movement or the movement of his feet. As can be seen from the [Homunculus](http://en.wikipedia.org/wiki/Cortical_homunculus), foot movement is controlled at the center of the motor cortex (which makes it hard to distinguish left from right foot), while hand movement is controlled more lateral.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data\n",
    "\n",
    "The code below cuts trials for the two classes and should look familiar if you've completed the previous tutorials. Trials are cut in the interval [0.5–2.5 s] after the onset of the cue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary to store the trials in, each class gets an entry\n",
    "trials = {}\n",
    "\n",
    "# The time window (in samples) to extract for each trial, here 0.5 -- 2.5 seconds\n",
    "win = np.arange(int(0.5*sample_rate), int(2.5*sample_rate))\n",
    "\n",
    "# Length of the time window\n",
    "nsamples = len(win)\n",
    "\n",
    "# Loop over the classes (right, foot)\n",
    "for cl, code in zip(cl_lab, np.unique(event_codes)):\n",
    "    \n",
    "    # Extract the onsets for the class\n",
    "    cl_onsets = event_onsets[event_codes == code]\n",
    "    \n",
    "    # Allocate memory for the trials\n",
    "    trials[cl] = np.zeros((nchannels, nsamples, len(cl_onsets)))\n",
    "    \n",
    "    # Extract each trial\n",
    "    for i, onset in enumerate(cl_onsets):\n",
    "        trials[cl][:,:,i] = EEG[:, win+onset]\n",
    "  \n",
    "# Some information about the dimensionality of the data (channels x time x trials)\n",
    "print 'Shape of trials[cl1]:', trials[cl1].shape\n",
    "print 'Shape of trials[cl2]:', trials[cl2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Since the feature we're looking for (a decrease in $\\mu$-activity) is a frequency feature, lets plot the PSD of the trials in a similar manner as with the SSVEP data. The code below defines a function that computes the PSD for each trial (we're going to need it again later on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import mlab\n",
    "\n",
    "def psd(trials):\n",
    "    '''\n",
    "    Calculates for each trial the Power Spectral Density (PSD).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trials : 3d-array (channels x samples x trials)\n",
    "        The EEG signal\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trial_PSD : 3d-array (channels x PSD x trials)\n",
    "        the PSD for each trial.  \n",
    "    freqs : list of floats\n",
    "        Yhe frequencies for which the PSD was computed (useful for plotting later)\n",
    "    '''\n",
    "    \n",
    "    ntrials = trials.shape[2]\n",
    "    trials_PSD = np.zeros((nchannels, 101, ntrials))\n",
    "\n",
    "    # Iterate over trials and channels\n",
    "    for trial in range(ntrials):\n",
    "        for ch in range(nchannels):\n",
    "            # Calculate the PSD\n",
    "            (PSD, freqs) = mlab.psd(trials[ch,:,trial], NFFT=int(nsamples), Fs=sample_rate)\n",
    "            trials_PSD[ch, :, trial] = PSD.ravel()\n",
    "                \n",
    "    return trials_PSD, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "psd_r, freqs = psd(trials[cl1])\n",
    "psd_f, freqs = psd(trials[cl2])\n",
    "trials_PSD = {cl1: psd_r, cl2: psd_f}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below plots the PSDs that are calculated with the above function. Since plotting it for 118 channels will clutter the display, it takes the indices of the desired channels as input, as well as some metadata to decorate the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_psd(trials_PSD, freqs, chan_ind, chan_lab=None, maxy=None):\n",
    "    '''\n",
    "    Plots PSD data calculated with psd().\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trials : 3d-array\n",
    "        The PSD data, as returned by psd()\n",
    "    freqs : list of floats\n",
    "        The frequencies for which the PSD is defined, as returned by psd() \n",
    "    chan_ind : list of integers\n",
    "        The indices of the channels to plot\n",
    "    chan_lab : list of strings\n",
    "        (optional) List of names for each channel\n",
    "    maxy : float\n",
    "        (optional) Limit the y-axis to this value\n",
    "    '''\n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    nchans = len(chan_ind)\n",
    "    \n",
    "    # Maximum of 3 plots per row\n",
    "    nrows = np.ceil(nchans / 3)\n",
    "    ncols = min(3, nchans)\n",
    "    \n",
    "    # Enumerate over the channels\n",
    "    for i,ch in enumerate(chan_ind):\n",
    "        # Figure out which subplot to draw to\n",
    "        plt.subplot(nrows,ncols,i+1)\n",
    "    \n",
    "        # Plot the PSD for each class\n",
    "        for cl in trials.keys():\n",
    "            plt.plot(freqs, np.mean(trials_PSD[cl][ch,:,:], axis=1), label=cl)\n",
    "    \n",
    "        # All plot decoration below...\n",
    "        \n",
    "        plt.xlim(1,30)\n",
    "        \n",
    "        if maxy != None:\n",
    "            plt.ylim(0,maxy)\n",
    "    \n",
    "        plt.grid()\n",
    "    \n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        \n",
    "        if chan_lab == None:\n",
    "            plt.title('Channel %d' % (ch+1))\n",
    "        else:\n",
    "            plt.title(chan_lab[i])\n",
    "\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put the `plot_psd()` function to use and plot three channels:\n",
    "\n",
    " 1. C3: Central, left\n",
    " 2. Cz: Central, central\n",
    " 3. C4: Central, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_psd(\n",
    "    trials_PSD,\n",
    "    freqs,\n",
    "    [channel_names.index(ch) for ch in ['C3', 'Cz', 'C4']],\n",
    "    chan_lab=['left', 'center', 'right'],\n",
    "    maxy=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A spike of mu activity can be seen on each channel for both classes. At the right hemisphere, the mu for the left hand movement is lower than for the right hand movement due to the ERD. At the left electrode, the mu for the right hand movement is reduced and at the central electrode the mu activity is about equal for both classes. This is in line with the theory that the left hand is controlled by the left hemiphere and the feet are controlled centrally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying the data\n",
    "\n",
    "We will use a machine learning algorithm to construct a model that can distinguish between the right hand and foot movement of this subject. In order to do this we need to:\n",
    "\n",
    " 1. find a way to quantify the amount of mu activity present in a trial\n",
    " 2. make a model that describes expected values of mu activity for each class\n",
    " 3. finally test this model on some unseen data to see if it can predict the correct class label\n",
    "\n",
    "We will follow a classic BCI design by Blankertz et al. [1] where they use the logarithm of the variance of the signal in a certain frequency band as a feature for the classifier.\n",
    "\n",
    "[1] Blankertz, B., Dornhege, G., Krauledat, M., Müller, K.-R., & Curio, G. (2007). The non-invasive Berlin Brain-Computer Interface: fast acquisition of effective performance in untrained subjects. *NeuroImage*, 37(2), 539–550. doi:10.1016/j.neuroimage.2007.01.051\n",
    "\n",
    "The script below designs a band pass filter using [`scipy.signal.irrfilter`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.iirfilter.html) that will strip away frequencies outside the 8--15Hz window. The filter is applied to all trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.signal \n",
    "\n",
    "def bandpass(trials, lo, hi, sample_rate):\n",
    "    '''\n",
    "    Designs and applies a bandpass filter to the signal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trials : 3d-array (channels x samples x trials)\n",
    "        The EEGsignal\n",
    "    lo : float\n",
    "        Lower frequency bound (in Hz)\n",
    "    hi : float\n",
    "        Upper frequency bound (in Hz)\n",
    "    sample_rate : float\n",
    "        Sample rate of the signal (in Hz)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trials_filt : 3d-array (channels x samples x trials)\n",
    "        The bandpassed signal\n",
    "    '''\n",
    "\n",
    "    # The iirfilter() function takes the filter order: higher numbers mean a sharper frequency cutoff,\n",
    "    # but the resulting signal might be shifted in time, lower numbers mean a soft frequency cutoff,\n",
    "    # but the resulting signal less distorted in time. It also takes the lower and upper frequency bounds\n",
    "    # to pass, divided by the niquist frequency, which is the sample rate divided by 2:\n",
    "    a, b = scipy.signal.iirfilter(6, [lo/(sample_rate/2.0), hi/(sample_rate/2.0)])\n",
    "\n",
    "    # Applying the filter to each trial\n",
    "    ntrials = trials.shape[2]\n",
    "    trials_filt = np.zeros((nchannels, nsamples, ntrials))\n",
    "    for i in range(ntrials):\n",
    "        trials_filt[:,:,i] = scipy.signal.filtfilt(a, b, trials[:,:,i], axis=1)\n",
    "    \n",
    "    return trials_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "trials_filt = {cl1: bandpass(trials[cl1], 8, 15, sample_rate),\n",
    "               cl2: bandpass(trials[cl2], 8, 15, sample_rate)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the PSD of the resulting `trials_filt` shows the suppression of frequencies outside the passband of the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "psd_r, freqs = psd(trials_filt[cl1])\n",
    "psd_f, freqs = psd(trials_filt[cl2])\n",
    "trials_PSD = {cl1: psd_r, cl2: psd_f}\n",
    "\n",
    "plot_psd(\n",
    "    trials_PSD,\n",
    "    freqs,\n",
    "    [channel_names.index(ch) for ch in ['C3', 'Cz', 'C4']],\n",
    "    chan_lab=['left', 'center', 'right'],\n",
    "    maxy=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a feature for the classifier, we will use the logarithm of the variance of each channel. The function below calculates this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the log(var) of the trials\n",
    "def logvar(trials):\n",
    "    '''\n",
    "    Calculate the log-var of each channel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trials : 3d-array (channels x samples x trials)\n",
    "        The EEG signal.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    logvar - 2d-array (channels x trials)\n",
    "        For each channel the logvar of the signal\n",
    "    '''\n",
    "    return np.log(np.var(trials, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "trials_logvar = {cl1: logvar(trials_filt[cl1]),\n",
    "                 cl2: logvar(trials_filt[cl2])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function to visualize the logvar of each channel as a bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_logvar(trials):\n",
    "    '''\n",
    "    Plots the log-var of each channel/component.\n",
    "    arguments:\n",
    "        trials - Dictionary containing the trials (log-vars x trials) for 2 classes.\n",
    "    '''\n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    x0 = np.arange(nchannels)\n",
    "    x1 = np.arange(nchannels) + 0.4\n",
    "\n",
    "    y0 = np.mean(trials[cl1], axis=1)\n",
    "    y1 = np.mean(trials[cl2], axis=1)\n",
    "\n",
    "    plt.bar(x0, y0, width=0.5, color='b')\n",
    "    plt.bar(x1, y1, width=0.4, color='r')\n",
    "\n",
    "    plt.xlim(-0.5, nchannels+0.5)\n",
    "\n",
    "    plt.gca().yaxis.grid(True)\n",
    "    plt.title('log-var of each channel/component')\n",
    "    plt.xlabel('channels/components')\n",
    "    plt.ylabel('log-var')\n",
    "    plt.legend(cl_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the log-vars\n",
    "plot_logvar(trials_logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most channels show a small difference in the log-var of the signal between the two classes. The next step is to go from 118 channels to only a few channel mixtures. The CSP algorithm calculates mixtures of channels that are designed to maximize the difference in variation between two classes. These mixures are called spatial filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "def cov(trials):\n",
    "    ''' Calculate the covariance for each trial and return their average '''\n",
    "    ntrials = trials.shape[2]\n",
    "    covs = [ trials[:,:,i].dot(trials[:,:,i].T) / nsamples for i in range(ntrials) ]\n",
    "    return np.mean(covs, axis=0)\n",
    "\n",
    "def whitening(sigma):\n",
    "    ''' Calculate a whitening matrix for covariance matrix sigma. '''\n",
    "    U, l, _ = linalg.svd(sigma)\n",
    "    return U.dot( np.diag(l ** -0.5) )\n",
    "\n",
    "def csp(trials_r, trials_f):\n",
    "    '''\n",
    "    Calculate the CSP transformation matrix W.\n",
    "    arguments:\n",
    "        trials_r - Array (channels x samples x trials) containing right hand movement trials\n",
    "        trials_f - Array (channels x samples x trials) containing foot movement trials\n",
    "    returns:\n",
    "        Mixing matrix W\n",
    "    '''\n",
    "    cov_r = cov(trials_r)\n",
    "    cov_f = cov(trials_f)\n",
    "    P = whitening(cov_r + cov_f)\n",
    "    B, _, _ = linalg.svd( P.T.dot(cov_f).dot(P) )\n",
    "    W = P.dot(B)\n",
    "    return W\n",
    "\n",
    "def apply_mix(W, trials):\n",
    "    ''' Apply a mixing matrix to each trial (basically multiply W with the EEG signal matrix)'''\n",
    "    ntrials = trials.shape[2]\n",
    "    trials_csp = np.zeros((nchannels, nsamples, ntrials))\n",
    "    for i in range(ntrials):\n",
    "        trials_csp[:,:,i] = W.T.dot(trials[:,:,i])\n",
    "    return trials_csp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the functions\n",
    "W = csp(trials_filt[cl1], trials_filt[cl2])\n",
    "trials_csp = {cl1: apply_mix(W, trials_filt[cl1]),\n",
    "              cl2: apply_mix(W, trials_filt[cl2])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the result of the CSP algorithm, we plot the log-var like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trials_logvar = {cl1: logvar(trials_csp[cl1]),\n",
    "                 cl2: logvar(trials_csp[cl2])}\n",
    "plot_logvar(trials_logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of 118 channels, we now have 118 mixtures of channels, called components. They are the result of 118 spatial filters applied to the data.\n",
    "\n",
    "The first filters maximize the variation of the first class, while minimizing the variation of the second. The last filters maximize the variation of the second class, while minimizing the variation of the first.\n",
    "\n",
    "This is also visible in a PSD plot. The code below plots the PSD for the first and last components as well as one in the middle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "psd_r, freqs = psd(trials_csp[cl1])\n",
    "psd_f, freqs = psd(trials_csp[cl2])\n",
    "trials_PSD = {cl1: psd_r, cl2: psd_f}\n",
    "\n",
    "plot_psd(trials_PSD, freqs, [0,58,-1], chan_lab=['first component', 'middle component', 'last component'], maxy=0.75 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how well we can differentiate between the two classes, a scatter plot is a useful tool. Here both classes are plotted on a 2-dimensional plane: the x-axis is the first CSP component, the y-axis is the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_scatter(left, foot):\n",
    "    plt.figure()\n",
    "    plt.scatter(left[0,:], left[-1,:], color='b')\n",
    "    plt.scatter(foot[0,:], foot[-1,:], color='r')\n",
    "    plt.xlabel('Last component')\n",
    "    plt.ylabel('First component')\n",
    "    plt.legend(cl_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_scatter(trials_logvar[cl1], trials_logvar[cl2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply a linear classifier to this data. A linear classifier can be thought of as drawing a line in the above plot to separate the two classes. To determine the class for a new trial, we just check on which side of the line the trial would be if plotted as above.\n",
    "\n",
    "The data is split into a train and a test set. The classifier will fit a model (in this case, a straight line) on the training set and use this model to make predictions about the test set (see on which side of the line each trial in the test set falls). Note that the CSP algorithm is part of the model, so for fairness sake it should be calculated using only the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Percentage of trials to use for training (50-50 split here)\n",
    "train_percentage = 0.5 \n",
    "\n",
    "# Calculate the number of trials for each class the above percentage boils down to\n",
    "ntrain_r = int(trials_filt[cl1].shape[2] * train_percentage)\n",
    "ntrain_f = int(trials_filt[cl2].shape[2] * train_percentage)\n",
    "ntest_r = trials_filt[cl1].shape[2] - ntrain_r\n",
    "ntest_f = trials_filt[cl2].shape[2] - ntrain_f\n",
    "\n",
    "# Splitting the frequency filtered signal into a train and test set\n",
    "train = {cl1: trials_filt[cl1][:,:,:ntrain_r],\n",
    "         cl2: trials_filt[cl2][:,:,:ntrain_f]}\n",
    "\n",
    "test = {cl1: trials_filt[cl1][:,:,ntrain_r:],\n",
    "        cl2: trials_filt[cl2][:,:,ntrain_f:]}\n",
    "\n",
    "# Train the CSP on the training set only\n",
    "W = csp(train[cl1], train[cl2])\n",
    "\n",
    "# Apply the CSP on both the training and test set\n",
    "train[cl1] = apply_mix(W, train[cl1])\n",
    "train[cl2] = apply_mix(W, train[cl2])\n",
    "test[cl1] = apply_mix(W, test[cl1])\n",
    "test[cl2] = apply_mix(W, test[cl2])\n",
    "\n",
    "# Select only the first and last components for classification\n",
    "comp = np.array([0,-1])\n",
    "train[cl1] = train[cl1][comp,:,:]\n",
    "train[cl2] = train[cl2][comp,:,:]\n",
    "test[cl1] = test[cl1][comp,:,:]\n",
    "test[cl2] = test[cl2][comp,:,:]\n",
    "\n",
    "# Calculate the log-var\n",
    "train[cl1] = logvar(train[cl1])\n",
    "train[cl2] = logvar(train[cl2])\n",
    "test[cl1] = logvar(test[cl1])\n",
    "test[cl2] = logvar(test[cl2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classifier the Linear Discriminant Analysis (LDA) algorithm will be used. It fits a gaussian distribution to each class, characterized by the mean and covariance, and determines an optimal separating plane to divide the two. This plane is defined as $r = W_0 \\cdot X_0 + W_1 \\cdot X_1 + \\ldots + W_n \\cdot X_n - b$, where $r$ is the classifier output, $W$ are called the feature weights, $X$ are the features of the trial, $n$ is the dimensionality of the data and $b$ is called the offset.\n",
    "\n",
    "In our case we have 2 dimensional data, so the separating plane will be a line: $r = W_0 \\cdot X_0 + W_1 \\cdot X_1 - b$. To determine a class label for an unseen trial, we can calculate whether the result is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_lda(class1, class2):\n",
    "    '''\n",
    "    Trains the LDA algorithm.\n",
    "    arguments:\n",
    "        class1 - An array (observations x features) for class 1\n",
    "        class2 - An array (observations x features) for class 2\n",
    "    returns:\n",
    "        The projection matrix W\n",
    "        The offset b\n",
    "    '''\n",
    "    nclasses = 2\n",
    "    \n",
    "    nclass1 = class1.shape[0]\n",
    "    nclass2 = class2.shape[0]\n",
    "    \n",
    "    # Class priors: in this case, we have an equal number of training\n",
    "    # examples for each class, so both priors are 0.5\n",
    "    prior1 = nclass1 / float(nclass1 + nclass2)\n",
    "    prior2 = nclass2 / float(nclass1 + nclass1)\n",
    "   \n",
    "    mean1 = np.mean(class1, axis=0)\n",
    "    mean2 = np.mean(class2, axis=0)\n",
    "    \n",
    "    class1_centered = class1 - mean1\n",
    "    class2_centered = class2 - mean2\n",
    "    \n",
    "    # Calculate the covariance between the features\n",
    "    cov1 = class1_centered.T.dot(class1_centered) / (nclass1 - nclasses)\n",
    "    cov2 = class2_centered.T.dot(class2_centered) / (nclass2 - nclasses)\n",
    "   \n",
    "    W = (mean2 - mean1).dot(np.linalg.pinv(prior1*cov1 + prior2*cov2))\n",
    "    b = (prior1*mean1 + prior2*mean2).dot(W)\n",
    "    \n",
    "    return (W,b)\n",
    "\n",
    "def apply_lda(test, W, b):\n",
    "    '''\n",
    "    Applies a previously trained LDA to new data.\n",
    "    arguments:\n",
    "        test - An array (features x trials) containing the data\n",
    "        W    - The project matrix W as calculated by train_lda()\n",
    "        b    - The offsets b as calculated by train_lda()\n",
    "    returns:\n",
    "        A list containing a classlabel for each trial\n",
    "    '''\n",
    "    ntrials = test.shape[1]\n",
    "    \n",
    "    prediction = []\n",
    "    for i in range(ntrials):\n",
    "        # The line below is a generalization for:\n",
    "        # result = W[0] * test[0,i] + W[1] * test[1,i] - b\n",
    "        result = W.dot(test[:,i]) - b\n",
    "        if result <= 0:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(2)\n",
    "    \n",
    "    return np.array(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the LDA using the training data gives us $W$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W,b = train_lda(train[cl1].T, train[cl2].T)\n",
    "\n",
    "print 'W:', W\n",
    "print 'b:', b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be informative to recreate the scatter plot and overlay the decision boundary as determined by the LDA classifier. The decision boundary is the line for which the classifier output is exactly 0. The scatterplot used $X_0$ as $x$-axis and $X_1$ as $y$-axis. To find the function $y = f(x)$ describing the decision boundary, we set $r$ to 0 and solve for $y$ in the equation of the separating plane:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:600px\">\n",
    "$$\\begin{align}\n",
    "W_0 \\cdot X_0 + W_1 \\cdot X_1 - b &= r &&\\text{the original equation} \\\\\\\n",
    "W_0 \\cdot x + W_1 \\cdot y - b &= 0     &&\\text{filling in $X_0=x$, $X_1=y$ and $r=0$} \\\\\\\n",
    "W_0 \\cdot x + W_1 \\cdot y &= b         &&\\text{solving for $y$}\\\\\\\n",
    "W_1 \\cdot y &= b - W_0 \\cdot x \\\\\\\n",
    "\\\\\\\n",
    "y &= \\frac{b - W_0 \\cdot x}{W_1}\n",
    "\\end{align}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first plot the decision boundary with the training data used to calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatterplot like before\n",
    "plot_scatter(train[cl1], train[cl2])\n",
    "title('Training data')\n",
    "\n",
    "# Calculate decision boundary (x,y)\n",
    "x = np.arange(-5, 1, 0.1)\n",
    "y = (b - W[0]*x) / W[1]\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.plot(x,y, linestyle='--', linewidth=2, color='k')\n",
    "plt.xlim(-5, 1)\n",
    "plt.ylim(-2.2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below plots the boundary with the test data on which we will apply the classifier. You will see the classifier is going to make some mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_scatter(test[cl1], test[cl2])\n",
    "title('Test data')\n",
    "plt.plot(x,y, linestyle='--', linewidth=2, color='k')\n",
    "plt.xlim(-5, 1)\n",
    "plt.ylim(-2.2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the LDA is constructed and fitted to the training data. We can now apply it to the test data. The results are presented as a confusion matrix:\n",
    "    \n",
    "<table>\n",
    "    <tr><td></td><td colspan='2' style=\"font-weight:bold\">True labels →</td></tr>\n",
    "    <tr><td style=\"font-weight:bold\">↓ Predicted labels</td><td>Right</td><td>Foot</td></tr>\n",
    "    <tr><td>Right</td><td></td><td></td></tr>\n",
    "    <tr><td>Foot</td><td></td><td></td></tr>\n",
    "</table>\n",
    "\n",
    "The number at the diagonal will be trials that were correctly classified, any trials incorrectly classified (either a false positive or false negative) will be in the corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print confusion matrix\n",
    "conf = np.array([\n",
    "    [(apply_lda(test[cl1], W, b) == 1).sum(), (apply_lda(test[cl2], W, b) == 1).sum()],\n",
    "    [(apply_lda(test[cl1], W, b) == 2).sum(), (apply_lda(test[cl2], W, b) == 2).sum()],\n",
    "])\n",
    "\n",
    "print 'Confusion matrix:'\n",
    "print conf\n",
    "print\n",
    "print 'Accuracy: %.3f' % (np.sum(np.diag(conf)) / float(np.sum(conf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that 4 out of the 50 trials with foot movement were incorrectly classified as right hand movement and 5 out of the 50 trials with right hand movement were incorrectly classified as foot movement. In total, 91% of the trials were correctly classified, not a bad score!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
